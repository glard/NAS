{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haha,got it work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why don't we start the project NAS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we need to find out how do the first paper build the Architecture and train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------- Get familiar with the code in autokeras-----------------------------#\n",
    "# ----------------classifier.py\n",
    "# init->fit->evaluate\n",
    "\n",
    "#fit->\n",
    "#    # Transform x,y_train.\n",
    "#    # Create the searcher and save on disk  \n",
    "#    # Divide training data into training and testing data\n",
    "#    # LOOP\n",
    "       $$$run_searcher_once(train_data, test_data, self.path)$$$\n",
    "         if len(self.load_searcher().history) >= Constant.MAX_MODEL_NUM#1000:\n",
    "                break\n",
    "#    #LOOP END\n",
    "#\n",
    "#\n",
    "#         # \n",
    "#         ###run_searcher_once()->autokeras.utils.pickle_from_file() CALL searcher.search()\n",
    "#             input_shape = x_train.shape[1:]\n",
    "            n_classes = self.y_encoder.n_classes #An integer, the number of classes\n",
    "            self.searcher_args['n_classes'] = n_classes #An integer, the number of classes\n",
    "            self.searcher_args['input_shape'] = input_shape #A tuple. e.g. (28, 28, 1)\n",
    "            self.searcher_args['path'] = self.path#The path to the directory to save the searcher\n",
    "            self.searcher_args['verbose'] = self.verbose#A boolean. Whether to output the intermediate information to stdout.\n",
    "            $$$$  searcher = BayesianSearcher(**self.searcher_args) $$$$\n",
    "            self.save_searcher(searcher)\n",
    "            self.searcher = True\n",
    "#-----------------Search.py\n",
    "\n",
    "#           ->__init__()\n",
    "                 default_model_len=Constant.MODEL_LEN,#3\n",
    "                 default_model_width=Constant.MODEL_WIDTH,#64\n",
    "                 beta=Constant.BETA,#0.576\n",
    "                 kernel_lambda=Constant.KERNEL_LAMBDA#1.0\n",
    "                if trainer_args is None:\n",
    "                    trainer_args = {}\n",
    "                if 'max_iter_num' not in self.trainer_args:\n",
    "                self.trainer_args['max_iter_num'] = Constant.SEARCH_MAX_ITER#200\n",
    "                self.gpr = IncrementalGaussianProcess(kernel_lambda)\n",
    "                self.search_tree = SearchTree()\n",
    "                self.training_queue = []\n",
    "                self.x_queue = []\n",
    "                self.y_queue = []\n",
    "                self.beta = beta\n",
    "                if t_min is None:\n",
    "                    t_min = Constant.T_MIN#0.0001\n",
    "                self.t_min = t_min\n",
    "#           ->search()\n",
    "#             ->init_search()  start\n",
    "#------------------generator.py\n",
    "                graph = DefaultClassifierGenerator(self.n_classes,\n",
    "                self.input_shape).generate(self.default_model_len,\n",
    "                                          self.default_model_width)\n",
    "                    $$$$$generate$$$$$\n",
    "                        model_width=Constant.MODEL_WIDTH#64):\n",
    "                        pooling_len = int(model_len#3 / 4)\n",
    "                        graph = Graph(self.input_shape, False)\n",
    "#------------------Graph.py                                          \n",
    "                          $$$$$$Graph.__init__start$$$$$$\n",
    "                                    self.weighted = weighted#A boolean of whether the weights and biases in the neural network should be included in the graph.\n",
    "                                    self.node_list = []#A list of integers, the indices of the list are the identifiers.\n",
    "                                    self.layer_list = []#A list of stub layers, the indices of the list are the identifiers.\n",
    "                                    # node id start with 0\n",
    "                                    self.node_to_id = {}#A dict instance mapping from node integers to their identifiers.\n",
    "                                    self.layer_to_id = {}# A dict instance mapping from stub layers to their identifiers.\n",
    "                                    self.layer_id_to_input_node_ids = {}#A dict instance mapping from layer identifiers to their input nodes identifiers.\n",
    "                                    self.layer_id_to_output_node_ids = {}\n",
    "                                    self.adj_list = {}#A two dimensional list. The adjacency list of the graph. The first dimension is identified by tensor identifiers. In each edge list, the elements are two-element tuples of (tensor identifier, layer identifier).\n",
    "                                    self.reverse_adj_list = {}#A reverse adjacent list in the same format as adj_list.\n",
    "                                    self.operation_history = []#A list saving all the network morphism operations.\n",
    "\n",
    "                                    self.vis = None#A dictionary of temporary storage for whether an local operation has been done during the network morphism.\n",
    "                                    self._add_node(Node(input_shape))\n",
    "                          $$$$$$Graph.__init__end$$$$$$\n",
    "                        temp_input_channel = self.input_shape[-1]\n",
    "                        output_node_id = 0\n",
    "                        for i in range(model_len):\n",
    "                            output_node_id = graph.add_layer(StubReLU(), output_node_id)\n",
    "                            output_node_id = graph.add_layer(StubConv(temp_input_channel, model_width, kernel_size=3), output_node_id)\n",
    "                            output_node_id = graph.add_layer(StubBatchNormalization(model_width), output_node_id)\n",
    "                            output_node_id = graph.add_layer(StubDropout(Constant.CONV_DROPOUT_RATE), output_node_id)\n",
    "                            temp_input_channel = model_width\n",
    "                            if pooling_len == 0 or ((i + 1) % pooling_len == 0 and i != model_len - 1):\n",
    "                                output_node_id = graph.add_layer(StubPooling(), output_node_id)\n",
    "\n",
    "                        output_node_id = graph.add_layer(StubFlatten(), output_node_id)\n",
    "                        output_node_id = graph.add_layer(StubDense(graph.node_list[output_node_id].shape[0], self.n_classes),\n",
    "                                                         output_node_id)\n",
    "                        graph.add_layer(StubSoftmax(), output_node_id)\n",
    "                        return graph\n",
    "                model_id = self.model_count\n",
    "                self.model_count += 1\n",
    "                self.training_queue.append((graph, -1, model_id))\n",
    "                self.descriptors.append(graph.extract_descriptor())\n",
    "                for child_graph in default_transform(graph):\n",
    "                    child_id = self.model_count\n",
    "                    self.model_count += 1\n",
    "                    self.training_queue.append((child_graph, model_id, child_id))\n",
    "                    self.descriptors.append(child_graph.extract_descriptor())\n",
    "                if self.verbose:\n",
    "                    print('Initialization finished.') \n",
    "#            ->init_search() end\n",
    "#            # Start the new process for training.\n",
    "            graph, father_id, model_id = self.training_queue.pop(0)\n",
    "            if self.verbose:\n",
    "                print('Training model ', model_id)\n",
    "                multiprocessing.set_start_method('spawn', force=True)\n",
    "                pool = multiprocessing.Pool(1)\n",
    "                $$$$$$$train_results = pool.map_async(train, [(graph, train_data, test_data, self.trainer_args,\n",
    "                                                        os.path.join(self.path, str(model_id) + '.png'), self.verbose)])                        \n",
    "#            # Do the search in current thread.\n",
    "            if not self.training_queue:\n",
    "                new_graph, new_father_id = self.maximize_acq()\n",
    "                new_model_id = self.model_count\n",
    "                self.model_count += 1\n",
    "                self.training_queue.append((new_graph, new_father_id, new_model_id))\n",
    "                descriptor = new_graph.extract_descriptor()\n",
    "                self.descriptors.append(new_graph.extract_descriptor())\n",
    "\n",
    "            accuracy, loss, graph = train_results.get()[0]\n",
    "            pool.terminate()\n",
    "            pool.join()\n",
    "            self.add_model(accuracy, loss, graph, model_id)\n",
    "            self.search_tree.add_child(father_id, model_id)\n",
    "            self.gpr.fit(self.x_queue, self.y_queue)\n",
    "            self.x_queue = []\n",
    "            self.y_queue = []\n",
    "\n",
    "            pickle_to_file(self, os.path.join(self.path, 'searcher'))\n",
    "            self.export_json(os.path.join(self.path, 'history.json'))\n",
    "\n",
    "\n",
    "# ->DefaultClassifierGenerator->Graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minst.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Minst dataset using autokeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing search.\n",
      "Plotting model init  1111111111\n",
      "Initialization finished.\n",
      "before the new proc\n",
      "Plotting model  0\n",
      "Training model  0\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from autokeras.classifier import ImageClassifier\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape(x_train.shape + (1,))\n",
    "    x_test = x_test.reshape(x_test.shape + (1,))\n",
    "\n",
    "    clf = ImageClassifier(verbose=True, augment=False)\n",
    "    clf.fit(x_train, y_train, time_limit=0.1 * 60 * 60)\n",
    "    clf.final_fit(x_train, y_train, x_test, y_test, retrain=True)\n",
    "    y = clf.evaluate(x_test, y_test)\n",
    "    print(y * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code alike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check keras status\n",
    "import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "# check new plot func \n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "\n",
    "\n",
    "#    plot_model(model,\n",
    "               to_file='model.png',\n",
    "               show_shapes=False,\n",
    "               show_layer_names=True,\n",
    "               rankdir='TB')\n",
    "#plot(model, to_file='model.png')                    \n",
    "    \"\"\"Converts a Keras model to dot format and save to a file.\n",
    "\n",
    "    # Arguments\n",
    "        model: A Keras model instance\n",
    "        to_file: File name of the plot image.\n",
    "        show_shapes: whether to display shape information.\n",
    "        show_layer_names: whether to display layer names.\n",
    "        rankdir: `rankdir` argument passed to PyDot,\n",
    "            a string specifying the format of the plot:\n",
    "            'TB' creates a vertical plot;\n",
    "            'LR' creates a horizontal plot.\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
